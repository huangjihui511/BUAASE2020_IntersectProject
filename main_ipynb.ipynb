{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.4-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python37464bitc618d87d24fd41819fe0e569b8dfcad8",
      "display_name": "Python 3.7.4 64-bit"
    },
    "colab": {
      "name": "“main.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huangjihui511/BUAASE2020_IntersectProject/blob/master/main_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "wJlHtdsJYLlj",
        "colab_type": "code",
        "outputId": "4f97440d-796c-485e-9ce6-d53a85191685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "path = \"COVID-Dialogue-Dataset-English.txt\"\n",
        "f = open(path,encoding=\"UTF-8\")\n",
        "l1 = f.read()\n",
        "l2 = l1.split(\"id=\")\n",
        "p2d = []\n",
        "d2p = []\n",
        "for item in l2:\n",
        "    if len(item) != 0:\n",
        "        temp = item.split(\"Dialogue\\nPatient:\")\n",
        "        if len(temp) > 1:\n",
        "            item2 = temp[1]\n",
        "            item3 = item2.split(\"Patient:\")\n",
        "            l = []\n",
        "            if len(item3) != 0:\n",
        "                for item4 in item3:\n",
        "                    item5 = item4.split(\"Doctor:\")\n",
        "                    l += item5\n",
        "            for i in range(1, len(l), 2):\n",
        "                p2d.append((l[i - 1].strip(\"\\n\"), l[i].strip(\"\\n\")))\n",
        "                if i + 1 < len(l):\n",
        "                    d2p.append((l[i].strip(\"\\n\"), l[i + 1].strip(\"\\n\")))\n",
        "data = p2d + d2p\n",
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Hello doctor, I get a cough for the last few days, which is heavy during night times. No raise in temperature but feeling tired with no travel history. No contact with any Covid-19 persons. It has been four to five days and has drunk a lot of Benadryl and took Paracetamol too. Doctors have shut the OP so do not know what to do? Please help.',\n",
              " 'Hello, I understand your concern. I just have a few more questions.\\nDoes your cough has phlegm? Any other symptoms like difficulty breathing? Any other medical condition such as asthma, hypertension? Are you a smoker? Alcoholic beverage drinker?')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQSDrfwYLlt",
        "colab_type": "code",
        "outputId": "13906333-e033-4d4c-cb5d-552f9cd68017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "dictionary = {}\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from itertools import chain\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "nltk.download('punkt')\n",
        "punct = punctuation\n",
        "reverse_flag = True\n",
        "\n",
        "def clean_data(data):\n",
        "    for item in data:\n",
        "        for item2 in item:\n",
        "            words = word_tokenize(item2)\n",
        "            for word in words:\n",
        "                dictionary.setdefault(word,0)\n",
        "                dictionary[word] += 1\n",
        "    data2 = []\n",
        "    for item in data:\n",
        "        temp = []\n",
        "        for item2 in item:\n",
        "            item2 = \" \".join([token for token in word_tokenize(item2) if dictionary[token] > 0 and token.lower() not in chain(punct, sw)])\n",
        "            if reverse_flag:\n",
        "                item2 = \" \".join(reversed(item2.split(\" \")))\n",
        "            temp.append(item2)\n",
        "        temp = tuple(temp)\n",
        "        data2.append(temp)\n",
        "    return data2\n",
        "data2 = clean_data(data)\n",
        "data2[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('help Please know OP shut Doctors Paracetamol took Benadryl lot drunk days five four persons Covid-19 contact history travel tired feeling temperature raise times night heavy days last cough get doctor Hello',\n",
              " 'drinker beverage Alcoholic smoker hypertension asthma condition medical breathing difficulty like symptoms phlegm cough questions concern understand Hello')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldaCgVmxYLlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_tO567xYLl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzgh9sz-YLl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j8IgYuNYLl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang):\n",
        "    print(\"Reading lines...\")\n",
        "    '''\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    '''\n",
        "    pairs = [(normalizeString(item[0]), normalizeString(item[1])) for item in data2]\n",
        "    # Reverse pairs, make Lang instances\n",
        "\n",
        "\n",
        "    lang = Lang(lang)\n",
        "\n",
        "    return lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaGsmxrIYLl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 1000\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "3w7_EuCTYLmC",
        "colab_type": "code",
        "outputId": "d8ca0455-0913-4005-8dc5-8f83bb382a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "def prepareData(lang):\n",
        "    lang, pairs = readLangs(lang)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        #print(pair)\n",
        "        lang.addSentence(pair[0])\n",
        "        lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(lang.name, lang.n_words)\n",
        "    return lang, pairs\n",
        "\n",
        "\n",
        "lang, pairs = prepareData('lang')\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 628 sentence pairs\n",
            "Trimmed to 628 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "lang 5283\n",
            "('covid tested medication responding days headaches throat sore degrees around temperature high daughter', 'w video maybe')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VQ8EgekYLmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnUCf1dfYLmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyMFF3U7YLmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYFwAirYYLmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU5YdZhVYLmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlSBpaNSYLmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpQciVvJYLmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkJzvzC8YLma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdRcgqgOYLmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgU6vmlsYLmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iRjv26iYLmh",
        "colab_type": "code",
        "outputId": "464ccd87-247e-491e-f7a0-5bf757fef187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_size = 128\n",
        "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 10000, print_every=500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 9s (- 31m 53s) (50 0%) 5.4034\n",
            "0m 16s (- 27m 1s) (100 1%) 4.0897\n",
            "0m 25s (- 27m 57s) (150 1%) 4.7177\n",
            "0m 33s (- 27m 24s) (200 2%) 4.5598\n",
            "0m 43s (- 28m 32s) (250 2%) 5.3923\n",
            "0m 55s (- 29m 48s) (300 3%) 5.9856\n",
            "1m 3s (- 29m 13s) (350 3%) 4.4751\n",
            "1m 16s (- 30m 35s) (400 4%) 5.4752\n",
            "1m 26s (- 30m 37s) (450 4%) 5.2837\n",
            "1m 34s (- 29m 58s) (500 5%) 4.9294\n",
            "1m 43s (- 29m 37s) (550 5%) 4.2398\n",
            "1m 53s (- 29m 31s) (600 6%) 5.0610\n",
            "2m 5s (- 29m 59s) (650 6%) 5.7830\n",
            "2m 14s (- 29m 44s) (700 7%) 4.9981\n",
            "2m 23s (- 29m 34s) (750 7%) 4.8577\n",
            "2m 32s (- 29m 16s) (800 8%) 5.2678\n",
            "2m 42s (- 29m 8s) (850 8%) 5.0635\n",
            "2m 50s (- 28m 46s) (900 9%) 4.7494\n",
            "3m 1s (- 28m 51s) (950 9%) 5.4268\n",
            "3m 12s (- 28m 49s) (1000 10%) 5.1545\n",
            "3m 23s (- 28m 53s) (1050 10%) 5.1437\n",
            "3m 33s (- 28m 50s) (1100 11%) 5.3195\n",
            "3m 45s (- 28m 54s) (1150 11%) 5.1719\n",
            "3m 53s (- 28m 29s) (1200 12%) 4.5779\n",
            "4m 3s (- 28m 26s) (1250 12%) 4.8539\n",
            "4m 12s (- 28m 11s) (1300 13%) 4.8452\n",
            "4m 22s (- 28m 4s) (1350 13%) 4.9664\n",
            "4m 34s (- 28m 7s) (1400 14%) 5.1022\n",
            "4m 45s (- 28m 3s) (1450 14%) 5.1582\n",
            "4m 54s (- 27m 50s) (1500 15%) 5.0230\n",
            "5m 6s (- 27m 50s) (1550 15%) 5.4910\n",
            "5m 16s (- 27m 43s) (1600 16%) 5.2671\n",
            "5m 27s (- 27m 34s) (1650 16%) 4.8632\n",
            "5m 36s (- 27m 21s) (1700 17%) 4.8829\n",
            "5m 44s (- 27m 4s) (1750 17%) 4.3429\n",
            "5m 53s (- 26m 51s) (1800 18%) 4.5749\n",
            "6m 3s (- 26m 41s) (1850 18%) 4.6737\n",
            "6m 14s (- 26m 34s) (1900 19%) 5.1410\n",
            "6m 24s (- 26m 29s) (1950 19%) 5.3277\n",
            "6m 35s (- 26m 22s) (2000 20%) 5.0660\n",
            "6m 46s (- 26m 15s) (2050 20%) 5.5672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a8f45d70b83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-29eaa6fdc28a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-77785d753587>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ],
        "id": "T0c0N_c5YLmj",
        "colab_type": "code",
        "outputId": "fe4aa28a-d0f6-4a63-ce7a-f71d921db29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> infection avoid take precautions water spreads covid morning daily swim use old years chaitanya hi\n",
            "= vaishalee dr assist know let helps hope travel avoid sickness home stay mask face cover water soap frequently hands wash gatherings social avoid include precautions minutes host find doesnot pool swimming killed get virus although sometime swimming refrain suggest would droplets air mainly spread human human however food spread suspected cases first advice query reviewed service doctor ask welcome hello\n",
            "< chat text video like would covid doctor doctor doctor covid <EOS>\n",
            "\n",
            "> chest sensation burning really sure university gibs covid exposure possible fever worse s upright stand cough sometimes cough like feels cough chest tight hi\n",
            "= chat text video like would medication ekg x ray chest possible need assess lungs listen person perhaps visit video assp seen worsens breathing recommendation office dr call best probably likely less covid cough fever without problems heart certain breath shortness tightness cause wheezing asthma ever asthma consider brief\n",
            "< chat text video like would covid doctor doctor covid <EOS>\n",
            "\n",
            "> help drinks foods recommendations rashes inflamed like looks maybe stuff red got throat like looks tongue behind middle tonsils though less pain smaller got sure better got tea water honey lot drank day one days swallowing pain tonsil throat sore week last condition serious anyone heard district near reports severe around cases confirmed close states united ca francisco san stay sleep enough get maybe sick little better bit feeling right chills least month next home staying hot really unless ac need might cold makes viruses possible allergies air clean purifier air turned recently online school lockdown places many since inside staying yes possible much smoking avoid try days asleep fall hour one around takes usually quicker asleep fall sleep recommendations medicine thanks week past coughs minor hear far pain coughs severe got one seems far woke ringing phone nearby sleep got worried sleeping trouble morning woke doctor thank\n",
            "= pack z need may revert promptly cases etc neck swelling malaise severe drip postnasal thick mucus thick pain increased fever symptoms warning watch wonderfully period tide help pain got time every tylenol additionally help weight appropriate zyrtec sudafed guess addition good makes tea water hot honey advise medicine forms indian traditional treatment supportive time better get usually corona viruses due infections worry infection common quite bumps granular could tonsils wall pharyngeal big slightly must guess size normal back get resolved problem problem throat sore infection current due enlarged could tonsils tonsils enlarged throat back bumps big two picked identity patient protect removed attachment interest area focussed around time job decent done camera yes virus wake canceled got meeting work india fly supposed brother sf cases heard tonight sleep good getting try soon alright precautions follow right got hi\n",
            "< chat text video like would covid doctor doctor covid <EOS>\n",
            "\n",
            "> advise please could times nauseous feel also aches body fever slight getting worse slightly getting symptoms week beginning throat sore slight chest tight suffering m hi\n",
            "= chat text video like would use alcohol washing hand measures hygiene strict adopt home stay others isolate event area risk high traveled diagnosed someone contact unless unlikely virus corona concerned everyone disprin water salt gargle paracetamol manage infection tract respiratory viral likely respiratory viral brief\n",
            "< chat text video like would covid doctor doctor covid doctor doctor doctor doctor covid <EOS>\n",
            "\n",
            "> headaches head loose feels brain neck well sore feels body whole dr . hi\n",
            "= chat text video like would allergic pain paracetamol use .only hotline tested .public get see screening nicd contact please viruses corona influenza like infection systemic could like sounds infec systemic viral brief\n",
            "< chat text video like would covid doctor doctor doctor doctor covid <EOS>\n",
            "\n",
            "> anyone contact travelled covid checked must cold common fever night last headache stomach uneasy cough flemmy occasional throat sore nose snotty well feeling son\n",
            "= chat text video like would change symptoms forum use right symptoms treat home medication use right tested indicated forums via consult telephonic get brief\n",
            "< chat text video like would covid doctor doctor covid <EOS>\n",
            "\n",
            "> pneumonia fine feel cough breaking cough today saturday believe started part coughing weeks couple throat clearing still cough gone temperature cough along sunday yesterday friday fever\n",
            "= thanks health good wish help happy query solved hope alright worry n t day times inhalation steam gargles water warm daily twice combination paracetamol levocetrizine take food chew hard avoid food junk avoid food spicy oily avoid hydrated keep orally fluids plenty drink gone .so also fever expectoration mucus pain chest less pneumonia possibility concern understand magic .i healthcare question thanks\n",
            "< chat text video like would covid doctor doctor covid doctor doctor covid <EOS>\n",
            "\n",
            "> corona prevent well work building office car filters filters hepa air filter hospitals know\n",
            "= chat text video like would others distance keeping outdoors getting encourages actually officials medical outdoors survive covid worries brief\n",
            "< chat text video like would covid doctor doctor covid <EOS>\n",
            "\n",
            "> yyyy yyyy longer much sure things loosen try mix mucinex added time twice doc cough hard probably blood assuming copper taste cough whenever punched someone like feel ribs better feel days three leaving fever levaquin rounds two ago weeks pneumonia bilateral developed treated point alpha positive\n",
            "= bhavsar kaushal dr . further .regards assist know let query answered hope mucinex continue cough easy made secretions dilutes actually pneumonia drug good mucinex subside definitely pain cured pneumonia chest areas affected pad water warm apply also tried ibuprofen paracetamol like drugs painkiller anti inflammatory pneumonia seen commonly pain chest pleura inflammation pleurisy causes advice .pneumonia query reviewed service .i doctor ask welcome hello\n",
            "< chat text video like would covid doctor doctor doctor covid doctor covid <EOS>\n",
            "\n",
            "> group part falls victoria zimbabwe visiting africa south back arrive virus corona tested get necessary\n",
            "= chat text video like would go way doctor cooperation quarantine self patient hospitalized sick reserved kits testing places tested get able desired tested symptoms infection coronavirus cough fever develop important precautions take infections coronavirus rate high place visited depends brief\n",
            "< chat text video like would covid doctor doctor doctor covid <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3oW_XXZYLmm",
        "colab_type": "code",
        "outputId": "c39042a7-29e7-45c7-bf87-eff0e9e3b7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lang.index2word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'SOS',\n",
              " 1: 'EOS',\n",
              " 2: 'hello',\n",
              " 3: 'doctor',\n",
              " 4: 'get',\n",
              " 5: 'cough',\n",
              " 6: 'last',\n",
              " 7: 'days',\n",
              " 8: 'heavy',\n",
              " 9: 'night',\n",
              " 10: 'times',\n",
              " 11: 'raise',\n",
              " 12: 'temperature',\n",
              " 13: 'feeling',\n",
              " 14: 'tired',\n",
              " 15: 'travel',\n",
              " 16: 'history',\n",
              " 17: 'contact',\n",
              " 18: 'covid',\n",
              " 19: 'persons',\n",
              " 20: 'four',\n",
              " 21: 'five',\n",
              " 22: 'drunk',\n",
              " 23: 'lot',\n",
              " 24: 'benadryl',\n",
              " 25: 'took',\n",
              " 26: 'paracetamol',\n",
              " 27: 'doctors',\n",
              " 28: 'shut',\n",
              " 29: 'op',\n",
              " 30: 'know',\n",
              " 31: 'please',\n",
              " 32: 'help',\n",
              " 33: 'understand',\n",
              " 34: 'concern',\n",
              " 35: 'questions',\n",
              " 36: 'phlegm',\n",
              " 37: 'symptoms',\n",
              " 38: 'like',\n",
              " 39: 'difficulty',\n",
              " 40: 'breathing',\n",
              " 41: 'medical',\n",
              " 42: 'condition',\n",
              " 43: 'asthma',\n",
              " 44: 'hypertension',\n",
              " 45: 'smoker',\n",
              " 46: 'alcoholic',\n",
              " 47: 'beverage',\n",
              " 48: 'drinker',\n",
              " 49: 'thank',\n",
              " 50: 'tiny',\n",
              " 51: 'amount',\n",
              " 52: 'comes',\n",
              " 53: 'time',\n",
              " 54: 'conditions',\n",
              " 55: 'hi',\n",
              " 56: 'would',\n",
              " 57: 'recommend',\n",
              " 58: 'take',\n",
              " 59: 'n',\n",
              " 60: 'acetylcysteine',\n",
              " 61: 'mg',\n",
              " 62: 'powder',\n",
              " 63: 'dissolved',\n",
              " 64: 'water',\n",
              " 65: 'three',\n",
              " 66: 'day',\n",
              " 67: 'may',\n",
              " 68: 'also',\n",
              " 69: 'nebulize',\n",
              " 70: 'using',\n",
              " 71: 'pnss',\n",
              " 72: 'saline',\n",
              " 73: 'nebulizer',\n",
              " 74: 'come',\n",
              " 75: 'vitamin',\n",
              " 76: 'c',\n",
              " 77: 'zinc',\n",
              " 78: 'boost',\n",
              " 79: 'immune',\n",
              " 80: 'system',\n",
              " 81: 'persist',\n",
              " 82: 'worsen',\n",
              " 83: 'new',\n",
              " 84: 'onset',\n",
              " 85: 'noted',\n",
              " 86: 'consult',\n",
              " 87: 'advised',\n",
              " 88: 'suffering',\n",
              " 89: 'coughing',\n",
              " 90: 'throat',\n",
              " 91: 'infection',\n",
              " 92: 'week',\n",
              " 93: 'fever',\n",
              " 94: 'felt',\n",
              " 95: 'chest',\n",
              " 96: 'pain',\n",
              " 97: 'two',\n",
              " 98: 'later',\n",
              " 99: 'consulted',\n",
              " 100: 'prescribed',\n",
              " 101: 'cavidur',\n",
              " 102: 'montek',\n",
              " 103: 'lc',\n",
              " 104: 'ambrolite',\n",
              " 105: 'syrup',\n",
              " 106: 'betaline',\n",
              " 107: 'gargle',\n",
              " 108: 'solution',\n",
              " 109: 'since',\n",
              " 110: 'improved',\n",
              " 111: 'frequent',\n",
              " 112: 'coming',\n",
              " 113: 'remarkably',\n",
              " 114: 'though',\n",
              " 115: 'completely',\n",
              " 116: 'yesterday',\n",
              " 117: 'onwards',\n",
              " 118: 'occuring',\n",
              " 119: 'maximum',\n",
              " 120: 'degree',\n",
              " 121: 'celcius',\n",
              " 122: 'touch',\n",
              " 123: 'foreign',\n",
              " 124: 'returned',\n",
              " 125: 'person',\n",
              " 126: 'went',\n",
              " 127: 'outside',\n",
              " 128: 'state',\n",
              " 129: 'incidence',\n",
              " 130: 'suggest',\n",
              " 131: 'opinion',\n",
              " 132: 'done',\n",
              " 133: 'x',\n",
              " 134: 'ray',\n",
              " 135: 'cbc',\n",
              " 136: 'complete',\n",
              " 137: 'blood',\n",
              " 138: 'count',\n",
              " 139: 'normal',\n",
              " 140: 'need',\n",
              " 141: 'worry',\n",
              " 142: 'much',\n",
              " 143: 'hope',\n",
              " 144: 'helps',\n",
              " 145: 'upload',\n",
              " 146: 'query',\n",
              " 147: 'yes',\n",
              " 148: 'see',\n",
              " 149: 'revert',\n",
              " 150: 'per',\n",
              " 151: 'guidelines',\n",
              " 152: 'made',\n",
              " 153: 'one',\n",
              " 154: 'test',\n",
              " 155: 'due',\n",
              " 156: 'city',\n",
              " 157: 'shutdown',\n",
              " 158: 'could',\n",
              " 159: 'able',\n",
              " 160: 'make',\n",
              " 161: 'every',\n",
              " 162: 'hours',\n",
              " 163: 'currently',\n",
              " 164: 'problem',\n",
              " 165: 'uploading',\n",
              " 166: 'examination',\n",
              " 167: 'report',\n",
              " 168: 'advice',\n",
              " 169: 'gone',\n",
              " 170: 'attached',\n",
              " 171: 'attachment',\n",
              " 172: 'removed',\n",
              " 173: 'protect',\n",
              " 174: 'patient',\n",
              " 175: 'identity',\n",
              " 176: 'total',\n",
              " 177: 'higher',\n",
              " 178: 'side',\n",
              " 179: 'along',\n",
              " 180: 'low',\n",
              " 181: 'hemoglobin',\n",
              " 182: 'start',\n",
              " 183: 'azithromycin',\n",
              " 184: 'tablet',\n",
              " 185: 'dolo',\n",
              " 186: 'subside',\n",
              " 187: 'iron',\n",
              " 188: 'many',\n",
              " 189: 'tablets',\n",
              " 190: 'taken',\n",
              " 191: 'morning',\n",
              " 192: 'want',\n",
              " 193: 'already',\n",
              " 194: 'today',\n",
              " 195: 'drink',\n",
              " 196: 'plenty',\n",
              " 197: 'fluids',\n",
              " 198: 'orally',\n",
              " 199: 'keep',\n",
              " 200: 'hydrated',\n",
              " 201: 'warm',\n",
              " 202: 'gargles',\n",
              " 203: 'steam',\n",
              " 204: 'inhalation',\n",
              " 205: 'year',\n",
              " 206: 'old',\n",
              " 207: 'man',\n",
              " 208: 'anxiety',\n",
              " 209: 'depression',\n",
              " 210: 'immunodeficiency',\n",
              " 211: 'disorders',\n",
              " 212: 'chronic',\n",
              " 213: 'diseases',\n",
              " 214: 'first',\n",
              " 215: 'wan',\n",
              " 216: 'na',\n",
              " 217: 'weakened',\n",
              " 218: 'likely',\n",
              " 219: 'die',\n",
              " 220: 'coronavirus',\n",
              " 221: 'second',\n",
              " 222: 'itchiness',\n",
              " 223: 'shortness',\n",
              " 224: 'breath',\n",
              " 225: 'always',\n",
              " 226: 'persistent',\n",
              " 227: 'feel',\n",
              " 228: 'subtle',\n",
              " 229: 'burning',\n",
              " 230: 'sensation',\n",
              " 231: 'go',\n",
              " 232: 'past',\n",
              " 233: 'ten',\n",
              " 234: 'someone',\n",
              " 235: 'positive',\n",
              " 236: 'quit',\n",
              " 237: 'smoking',\n",
              " 238: 'zero',\n",
              " 239: 'cigarettes',\n",
              " 240: 'manifest',\n",
              " 241: 'physical',\n",
              " 242: 'psychological',\n",
              " 243: 'irritation',\n",
              " 244: 'experiencing',\n",
              " 245: 'part',\n",
              " 246: 'believe',\n",
              " 247: 'hype',\n",
              " 248: 'mortality',\n",
              " 249: 'rate',\n",
              " 250: 'percent',\n",
              " 251: 'high',\n",
              " 252: 'people',\n",
              " 253: 'years',\n",
              " 254: 'co',\n",
              " 255: 'morbidities',\n",
              " 256: 'even',\n",
              " 257: 'anyone',\n",
              " 258: 'continue',\n",
              " 259: 'medicines',\n",
              " 260: 'taking',\n",
              " 261: 'consider',\n",
              " 262: 'visiting',\n",
              " 263: 'psychiatrist',\n",
              " 264: 'started',\n",
              " 265: 'dose',\n",
              " 266: 'ssri',\n",
              " 267: 'type',\n",
              " 268: 'needed',\n",
              " 269: 'benzodiazepines',\n",
              " 270: 'added',\n",
              " 271: 'temporarily',\n",
              " 272: 'deep',\n",
              " 273: 'exercises',\n",
              " 274: 'progressive',\n",
              " 275: 'muscle',\n",
              " 276: 'relaxation',\n",
              " 277: 'honey',\n",
              " 278: 'reduce',\n",
              " 279: 'itching',\n",
              " 280: 'try',\n",
              " 281: 'lukewarm',\n",
              " 282: 'salt',\n",
              " 283: 'getting',\n",
              " 284: 'chills',\n",
              " 285: 'cold',\n",
              " 286: 'usual',\n",
              " 287: 'lightheaded',\n",
              " 288: 'burping',\n",
              " 289: 'maybe',\n",
              " 290: 'minutes',\n",
              " 291: 'less',\n",
              " 292: 'running',\n",
              " 293: 'body',\n",
              " 294: 'bit',\n",
              " 295: 'weird',\n",
              " 296: 'starting',\n",
              " 297: 'trouble',\n",
              " 298: 'goes',\n",
              " 299: 'back',\n",
              " 300: 'repeats',\n",
              " 301: 'left',\n",
              " 302: 'right',\n",
              " 303: 'lungs',\n",
              " 304: 'upper',\n",
              " 305: 'heart',\n",
              " 306: 'pumping',\n",
              " 307: 'fast',\n",
              " 308: 'minor',\n",
              " 309: 'coughs',\n",
              " 310: 'seven',\n",
              " 311: 'ago',\n",
              " 312: 'still',\n",
              " 313: 'white',\n",
              " 314: 'mucus',\n",
              " 315: 'gotten',\n",
              " 316: 'yellow',\n",
              " 317: 'yet',\n",
              " 318: 'got',\n",
              " 319: 'severe',\n",
              " 320: 'flu',\n",
              " 321: 'medicine',\n",
              " 322: 'reliever',\n",
              " 323: 'away',\n",
              " 324: 'called',\n",
              " 325: 'phone',\n",
              " 326: 'told',\n",
              " 327: 'happen',\n",
              " 328: 'thing',\n",
              " 329: 'happened',\n",
              " 330: 'breathe',\n",
              " 331: 'something',\n",
              " 332: 'issue',\n",
              " 333: 'going',\n",
              " 334: 'least',\n",
              " 335: 'somehow',\n",
              " 336: 'gives',\n",
              " 337: 'pepcid',\n",
              " 338: 'gerd',\n",
              " 339: 'ate',\n",
              " 340: 'arms',\n",
              " 341: 'makes',\n",
              " 342: 'arm',\n",
              " 343: 'sometimes',\n",
              " 344: 'worried',\n",
              " 345: 'think',\n",
              " 346: 'hot',\n",
              " 347: 'forehead',\n",
              " 348: 'else',\n",
              " 349: 'weigh',\n",
              " 350: 'around',\n",
              " 351: 'kg',\n",
              " 352: 'answer',\n",
              " 353: 'following',\n",
              " 354: 'month',\n",
              " 355: 'use',\n",
              " 356: 'airports',\n",
              " 357: 'checked',\n",
              " 358: 'sleep',\n",
              " 359: 'malaise',\n",
              " 360: 'aches',\n",
              " 361: 'weakness',\n",
              " 362: 'school',\n",
              " 363: 'nearby',\n",
              " 364: 'grocery',\n",
              " 365: 'stores',\n",
              " 366: 'bus',\n",
              " 367: 'airport',\n",
              " 368: 'exposure',\n",
              " 369: 'anything',\n",
              " 370: 'sore',\n",
              " 371: 'seems',\n",
              " 372: 'clear',\n",
              " 373: 'probably',\n",
              " 374: 'thinking',\n",
              " 375: 'stuff',\n",
              " 376: 'appear',\n",
              " 377: 'whole',\n",
              " 378: 'smoke',\n",
              " 379: 'sudafed',\n",
              " 380: 'tylenol',\n",
              " 381: 'zyrtec',\n",
              " 382: 'sure',\n",
              " 383: 'better',\n",
              " 384: 'avoid',\n",
              " 385: 'dust',\n",
              " 386: 'winds',\n",
              " 387: 'ac',\n",
              " 388: 'minimum',\n",
              " 389: 'ceiling',\n",
              " 390: 'fan',\n",
              " 391: 'instead',\n",
              " 392: 'eating',\n",
              " 393: 'partying',\n",
              " 394: 'unnecessary',\n",
              " 395: 'quarantine',\n",
              " 396: 'strictest',\n",
              " 397: 'manner',\n",
              " 398: 'restrictions',\n",
              " 399: 'interacting',\n",
              " 400: 'meeting',\n",
              " 401: 'community',\n",
              " 402: 'lastly',\n",
              " 403: 'place',\n",
              " 404: 'residence',\n",
              " 405: 'country',\n",
              " 406: 'corona',\n",
              " 407: 'endemic',\n",
              " 408: 'talk',\n",
              " 409: 'interact',\n",
              " 410: 'known',\n",
              " 411: 'suspected',\n",
              " 412: 'picture',\n",
              " 413: 'open',\n",
              " 414: 'mouth',\n",
              " 415: 'showing',\n",
              " 416: 'details',\n",
              " 417: 'tonsils',\n",
              " 418: 'pharyngeal',\n",
              " 419: 'wall',\n",
              " 420: 'send',\n",
              " 421: 'woke',\n",
              " 422: 'sleeping',\n",
              " 423: 'ringing',\n",
              " 424: 'far',\n",
              " 425: 'hear',\n",
              " 426: 'thanks',\n",
              " 427: 'recommendations',\n",
              " 428: 'fall',\n",
              " 429: 'asleep',\n",
              " 430: 'quicker',\n",
              " 431: 'usually',\n",
              " 432: 'takes',\n",
              " 433: 'hour',\n",
              " 434: 'possible',\n",
              " 435: 'staying',\n",
              " 436: 'inside',\n",
              " 437: 'places',\n",
              " 438: 'lockdown',\n",
              " 439: 'online',\n",
              " 440: 'recently',\n",
              " 441: 'turned',\n",
              " 442: 'air',\n",
              " 443: 'purifier',\n",
              " 444: 'clean',\n",
              " 445: 'allergies',\n",
              " 446: 'viruses',\n",
              " 447: 'might',\n",
              " 448: 'unless',\n",
              " 449: 'really',\n",
              " 450: 'home',\n",
              " 451: 'next',\n",
              " 452: 'little',\n",
              " 453: 'sick',\n",
              " 454: 'enough',\n",
              " 455: 'stay',\n",
              " 456: 'san',\n",
              " 457: 'francisco',\n",
              " 458: 'ca',\n",
              " 459: 'united',\n",
              " 460: 'states',\n",
              " 461: 'close',\n",
              " 462: 'confirmed',\n",
              " 463: 'cases',\n",
              " 464: 'reports',\n",
              " 465: 'near',\n",
              " 466: 'district',\n",
              " 467: 'heard',\n",
              " 468: 'serious',\n",
              " 469: 'tonsil',\n",
              " 470: 'swallowing',\n",
              " 471: 'drank',\n",
              " 472: 'tea',\n",
              " 473: 'smaller',\n",
              " 474: 'middle',\n",
              " 475: 'behind',\n",
              " 476: 'tongue',\n",
              " 477: 'looks',\n",
              " 478: 'red',\n",
              " 479: 'inflamed',\n",
              " 480: 'rashes',\n",
              " 481: 'foods',\n",
              " 482: 'drinks',\n",
              " 483: 'follow',\n",
              " 484: 'precautions',\n",
              " 485: 'alright',\n",
              " 486: 'soon',\n",
              " 487: 'good',\n",
              " 488: 'tonight',\n",
              " 489: 'sf',\n",
              " 490: 'brother',\n",
              " 491: 'supposed',\n",
              " 492: 'fly',\n",
              " 493: 'india',\n",
              " 494: 'work',\n",
              " 495: 'canceled',\n",
              " 496: 'wake',\n",
              " 497: 'virus',\n",
              " 498: 'camera',\n",
              " 499: 'decent',\n",
              " 500: 'job',\n",
              " 501: 'focussed',\n",
              " 502: 'area',\n",
              " 503: 'interest',\n",
              " 504: 'picked',\n",
              " 505: 'big',\n",
              " 506: 'bumps',\n",
              " 507: 'enlarged',\n",
              " 508: 'current',\n",
              " 509: 'resolved',\n",
              " 510: 'size',\n",
              " 511: 'guess',\n",
              " 512: 'must',\n",
              " 513: 'slightly',\n",
              " 514: 'granular',\n",
              " 515: 'quite',\n",
              " 516: 'common',\n",
              " 517: 'infections',\n",
              " 518: 'supportive',\n",
              " 519: 'treatment',\n",
              " 520: 'traditional',\n",
              " 521: 'indian',\n",
              " 522: 'forms',\n",
              " 523: 'advise',\n",
              " 524: 'addition',\n",
              " 525: 'appropriate',\n",
              " 526: 'weight',\n",
              " 527: 'additionally',\n",
              " 528: 'tide',\n",
              " 529: 'period',\n",
              " 530: 'wonderfully',\n",
              " 531: 'watch',\n",
              " 532: 'warning',\n",
              " 533: 'increased',\n",
              " 534: 'thick',\n",
              " 535: 'postnasal',\n",
              " 536: 'drip',\n",
              " 537: 'swelling',\n",
              " 538: 'neck',\n",
              " 539: 'etc',\n",
              " 540: 'promptly',\n",
              " 541: 'z',\n",
              " 542: 'pack',\n",
              " 543: 'pm',\n",
              " 544: 'harder',\n",
              " 545: 'breakfast',\n",
              " 546: 'story',\n",
              " 547: 'feb',\n",
              " 548: 'th',\n",
              " 549: 'issues',\n",
              " 550: 'besides',\n",
              " 551: 'bloating',\n",
              " 552: 'lightheadedness',\n",
              " 553: 'problems',\n",
              " 554: 'friday',\n",
              " 555: 'february',\n",
              " 556: 'unwell',\n",
              " 557: 'bed',\n",
              " 558: 'came',\n",
              " 559: 'super',\n",
              " 560: 'faint',\n",
              " 561: 'lied',\n",
              " 562: 'stomach',\n",
              " 563: 'barely',\n",
              " 564: 'eat',\n",
              " 565: 'noodles',\n",
              " 566: 'small',\n",
              " 567: 'pieces',\n",
              " 568: 'spam',\n",
              " 569: 'vomited',\n",
              " 570: 'acid',\n",
              " 571: 'saturday',\n",
              " 572: 'diarrhea',\n",
              " 573: 'sunday',\n",
              " 574: 'march',\n",
              " 575: 'st',\n",
              " 576: 'monday',\n",
              " 577: 'tuesday',\n",
              " 578: 'wednesday',\n",
              " 579: 'stool',\n",
              " 580: 'thursday',\n",
              " 581: 'afternoon',\n",
              " 582: 'chicken',\n",
              " 583: 'uncomfortable',\n",
              " 584: 'hands',\n",
              " 585: 'kind',\n",
              " 586: 'fainting',\n",
              " 587: 'randomly',\n",
              " 588: 'tums',\n",
              " 589: 'described',\n",
              " 590: 'said',\n",
              " 591: 'sounds',\n",
              " 592: 'bad',\n",
              " 593: 'sound',\n",
              " 594: 'course',\n",
              " 595: 'lab',\n",
              " 596: 'tests',\n",
              " 597: 'asked',\n",
              " 598: 'tried',\n",
              " 599: 'stronger',\n",
              " 600: 'antacids',\n",
              " 601: 'zantac',\n",
              " 602: 'tell',\n",
              " 603: 'making',\n",
              " 604: 'burp',\n",
              " 605: 'almost',\n",
              " 606: 'fine',\n",
              " 607: 'office',\n",
              " 608: 'beats',\n",
              " 609: 'minute',\n",
              " 610: 'counted',\n",
              " 611: 'rating',\n",
              " 612: 'bpm',\n",
              " 613: 'call',\n",
              " 614: 'gets',\n",
              " 615: 'worse',\n",
              " 616: 'shortly',\n",
              " 617: 'drop',\n",
              " 618: 'slowly',\n",
              " 619: 'pains',\n",
              " 620: 'beating',\n",
              " 621: 'moving',\n",
              " 622: 'sat',\n",
              " 623: 'strange',\n",
              " 624: 'reflux',\n",
              " 625: 'esophagus',\n",
              " 626: 'protonix',\n",
              " 627: 'pantoprazole',\n",
              " 628: 'lie',\n",
              " 629: 'immediately',\n",
              " 630: 'consuming',\n",
              " 631: 'meals',\n",
              " 632: 'meal',\n",
              " 633: 'spicy',\n",
              " 634: 'oily',\n",
              " 635: 'store',\n",
              " 636: 'look',\n",
              " 637: 'talked',\n",
              " 638: 'pharmacy',\n",
              " 639: 'tightening',\n",
              " 640: 'ranitidine',\n",
              " 641: 'means',\n",
              " 642: 'nervous',\n",
              " 643: 'calm',\n",
              " 644: 'putting',\n",
              " 645: 'fingers',\n",
              " 646: 'teeth',\n",
              " 647: 'cause',\n",
              " 648: 'beat',\n",
              " 649: 'faster',\n",
              " 650: 'stick',\n",
              " 651: 'finish',\n",
              " 652: 'entire',\n",
              " 653: 'bottle',\n",
              " 654: 'used',\n",
              " 655: 'hungry',\n",
              " 656: 'snacks',\n",
              " 657: 'lying',\n",
              " 658: 'starve',\n",
              " 659: 't',\n",
              " 660: 'lasted',\n",
              " 661: 'waited',\n",
              " 662: 'pressure',\n",
              " 663: 'proton',\n",
              " 664: 'pump',\n",
              " 665: 'inhibitor',\n",
              " 666: 'appears',\n",
              " 667: 'heartburn',\n",
              " 668: 'acidity',\n",
              " 669: 'sinister',\n",
              " 670: 'trying',\n",
              " 671: 'bowl',\n",
              " 672: 'food',\n",
              " 673: 'veggies',\n",
              " 674: 'rice',\n",
              " 675: 'halfway',\n",
              " 676: 'meat',\n",
              " 677: 'coughed',\n",
              " 678: 'well',\n",
              " 679: 'cooked',\n",
              " 680: 'meats',\n",
              " 681: 'appreciate',\n",
              " 682: 'effort',\n",
              " 683: 'give',\n",
              " 684: 'wait',\n",
              " 685: 'drinking',\n",
              " 686: 'absolutely',\n",
              " 687: 'laryngoscopy',\n",
              " 688: 'check',\n",
              " 689: 'wrong',\n",
              " 690: 'larynx',\n",
              " 691: 'healthy',\n",
              " 692: 'yeah',\n",
              " 693: 'perfect',\n",
              " 694: 'okay',\n",
              " 695: 'll',\n",
              " 696: 'vegetables',\n",
              " 697: 'fruits',\n",
              " 698: 'junk',\n",
              " 699: 'mostly',\n",
              " 700: 'pork',\n",
              " 701: 'beef',\n",
              " 702: 'fish',\n",
              " 703: 'fried',\n",
              " 704: 'wheat',\n",
              " 705: 'bread',\n",
              " 706: 'patty',\n",
              " 707: 'market',\n",
              " 708: 'lunch',\n",
              " 709: 'cheap',\n",
              " 710: 'stopped',\n",
              " 711: 'french',\n",
              " 712: 'fries',\n",
              " 713: 'pizza',\n",
              " 714: 'soda',\n",
              " 715: '.',\n",
              " 716: 'exercise',\n",
              " 717: 'exercising',\n",
              " 718: 'months',\n",
              " 719: 'telling',\n",
              " 720: 'feels',\n",
              " 721: 'tight',\n",
              " 722: 'squeeze',\n",
              " 723: 'way',\n",
              " 724: 'point',\n",
              " 725: 'college',\n",
              " 726: 'm',\n",
              " 727: 'younger',\n",
              " 728: 'elementary',\n",
              " 729: 'squeezes',\n",
              " 730: 'stethoscope',\n",
              " 731: 'another',\n",
              " 732: 'regularly',\n",
              " 733: '.that',\n",
              " 734: 's',\n",
              " 735: 'recent',\n",
              " 736: 'kinda',\n",
              " 737: 'anyways',\n",
              " 738: 'track',\n",
              " 739: 'regards',\n",
              " 740: 'diet',\n",
              " 741: 'restart',\n",
              " 742: 'active',\n",
              " 743: 'quantity',\n",
              " 744: 'frequently',\n",
              " 745: 'without',\n",
              " 746: 'adding',\n",
              " 747: 'calories',\n",
              " 748: 'chaitanya',\n",
              " 749: 'swim',\n",
              " 750: 'daily',\n",
              " 751: 'spreads',\n",
              " 752: 'welcome',\n",
              " 753: 'ask',\n",
              " 754: 'service',\n",
              " 755: 'reviewed',\n",
              " 756: 'spread',\n",
              " 757: 'however',\n",
              " 758: 'human',\n",
              " 759: 'mainly',\n",
              " 760: 'droplets',\n",
              " 761: 'refrain',\n",
              " 762: 'swimming',\n",
              " 763: 'sometime',\n",
              " 764: 'although',\n",
              " 765: 'killed',\n",
              " 766: 'pool',\n",
              " 767: 'doesnot',\n",
              " 768: 'find',\n",
              " 769: 'host',\n",
              " 770: 'include',\n",
              " 771: 'social',\n",
              " 772: 'gatherings',\n",
              " 773: 'wash',\n",
              " 774: 'soap',\n",
              " 775: 'cover',\n",
              " 776: 'face',\n",
              " 777: 'mask',\n",
              " 778: 'sickness',\n",
              " 779: 'let',\n",
              " 780: 'assist',\n",
              " 781: 'dr',\n",
              " 782: 'vaishalee',\n",
              " 783: '',\n",
              " 784: 'depends',\n",
              " 785: 'severity',\n",
              " 786: 'pandemic',\n",
              " 787: 'video',\n",
              " 788: 'requiring',\n",
              " 789: 'in',\n",
              " 790: 'visit',\n",
              " 791: 'strep',\n",
              " 792: 'influenza',\n",
              " 793: 'calls',\n",
              " 794: 'bothersome',\n",
              " 795: 'recurrent',\n",
              " 796: 'testing',\n",
              " 797: 'local',\n",
              " 798: 'availability',\n",
              " 799: 'criteria',\n",
              " 800: 'real',\n",
              " 801: 'vs',\n",
              " 802: 'textbook',\n",
              " 803: 'life',\n",
              " 804: 'different',\n",
              " 805: 'supplies',\n",
              " 806: 'staff',\n",
              " 807: 'world',\n",
              " 808: 'hospitalized',\n",
              " 809: 'underlying',\n",
              " 810: 'suspect',\n",
              " 811: 'risk',\n",
              " 812: 'impossible',\n",
              " 813: 'say',\n",
              " 814: 'vomiting',\n",
              " 815: 'symptom',\n",
              " 816: 'nausea',\n",
              " 817: 'and',\n",
              " 818: 'or',\n",
              " 819: 'occur',\n",
              " 820: 'fairly',\n",
              " 821: 'early',\n",
              " 822: 'rarely',\n",
              " 823: 'ever',\n",
              " 824: 'develop',\n",
              " 825: 'within',\n",
              " 826: 'news',\n",
              " 827: 'break',\n",
              " 828: 'strategy',\n",
              " 829: 'information',\n",
              " 830: 'practice',\n",
              " 831: 'prudent',\n",
              " 832: 'er',\n",
              " 833: 'practicing',\n",
              " 834: 'front',\n",
              " 835: 'line',\n",
              " 836: 'focus',\n",
              " 837: 'mind',\n",
              " 838: 'cooking',\n",
              " 839: 'planning',\n",
              " 840: 'garden',\n",
              " 841: 'engage',\n",
              " 842: 'friends',\n",
              " 843: 'loved',\n",
              " 844: 'ones',\n",
              " 845: 'read',\n",
              " 846: 'e',\n",
              " 847: 'cov',\n",
              " 848: 'several',\n",
              " 849: 'coronaviruses',\n",
              " 850: 'colds',\n",
              " 851: 'biologically',\n",
              " 852: 'related',\n",
              " 853: 'sars',\n",
              " 854: 'seek',\n",
              " 855: 'care',\n",
              " 856: 'plus',\n",
              " 857: 'facility',\n",
              " 858: 'treating',\n",
              " 859: 'pneumonia',\n",
              " 860: 'unknown',\n",
              " 861: 'possibly',\n",
              " 862: 'exposed',\n",
              " 863: 'consultation',\n",
              " 864: 'self',\n",
              " 865: 'isolate',\n",
              " 866: 'tested',\n",
              " 867: 'health',\n",
              " 868: 'department',\n",
              " 869: 'idea',\n",
              " 870: 'set',\n",
              " 871: 'live',\n",
              " 872: 'pick',\n",
              " 873: 'weeks',\n",
              " 874: 'us',\n",
              " 875: 'avoiding',\n",
              " 876: 'hand',\n",
              " 877: 'washing',\n",
              " 878: 'important',\n",
              " 879: 'prophylactic',\n",
              " 880: 'measures',\n",
              " 881: 'dad',\n",
              " 882: 'runny',\n",
              " 883: 'nose',\n",
              " 884: '.any',\n",
              " 885: 'chance',\n",
              " 886: 'uncommon',\n",
              " 887: 'worldwide',\n",
              " 888: 'goal',\n",
              " 889: 'separated',\n",
              " 890: 'clinics',\n",
              " 891: 'hospitals',\n",
              " 892: 'overwhelmed',\n",
              " 893: 'patients',\n",
              " 894: 'primary',\n",
              " 895: 'instructed',\n",
              " 896: 'clinic',\n",
              " 897: 'treatable',\n",
              " 898: 'headache',\n",
              " 899: 'earache',\n",
              " 900: 'pattern',\n",
              " 901: 'specific',\n",
              " 902: 'agent',\n",
              " 903: 'suggests',\n",
              " 904: 'ear',\n",
              " 905: 'kids',\n",
              " 906: 'among',\n",
              " 907: 'affected',\n",
              " 908: 'proper',\n",
              " 909: 'screening',\n",
              " 910: 'confirm',\n",
              " 911: 'exclude',\n",
              " 912: 'parainfluenza',\n",
              " 913: 'rhinovirus',\n",
              " 914: 'rsv',\n",
              " 915: 'battling',\n",
              " 916: 'post',\n",
              " 917: 'nasal',\n",
              " 918: 'laproscope',\n",
              " 919: 'removal',\n",
              " 920: 'fallopian',\n",
              " 921: 'tubes',\n",
              " 922: 'nd',\n",
              " 923: 'o',\n",
              " 924: 'brief',\n",
              " 925: 'unlikely',\n",
              " 926: 'treat',\n",
              " 927: 'symptomatically',\n",
              " 928: 'stock',\n",
              " 929: 'whilst',\n",
              " 930: 'qualify',\n",
              " 931: 'posting',\n",
              " 932: 'true',\n",
              " 933: 'false',\n",
              " 934: 'infected',\n",
              " 935: 'increases',\n",
              " 936: 'chances',\n",
              " 937: 'death',\n",
              " 938: 'compared',\n",
              " 939: 'respiratory',\n",
              " 940: 'data',\n",
              " 941: 'china',\n",
              " 942: 'association',\n",
              " 943: 'inflammatory',\n",
              " 944: 'response',\n",
              " 945: 'associated',\n",
              " 946: 'portion',\n",
              " 947: 'illness',\n",
              " 948: 'sx',\n",
              " 949: 'clinical',\n",
              " 950: 'characteristics',\n",
              " 951: 'novel',\n",
              " 952: 'wuhan',\n",
              " 953: 'jama',\n",
              " 954: 'wang',\n",
              " 955: 'et',\n",
              " 956: 'al',\n",
              " 957: 'study',\n",
              " 958: 'looked',\n",
              " 959: 'number',\n",
              " 960: 'transmitted',\n",
              " 961: 'sexually',\n",
              " 962: 'sort',\n",
              " 963: 'sex',\n",
              " 964: 'likelihood',\n",
              " 965: 'kissing',\n",
              " 966: 'intercourse',\n",
              " 967: 'closeness',\n",
              " 968: 'intimate',\n",
              " 969: 'situation',\n",
              " 970: 'transmission',\n",
              " 971: 'shave',\n",
              " 972: 'beard',\n",
              " 973: 'contracting',\n",
              " 974: 'clings',\n",
              " 975: 'everything',\n",
              " 976: 'especially',\n",
              " 977: 'hairs',\n",
              " 978: 'either',\n",
              " 979: 'spray',\n",
              " 980: 'alcohol',\n",
              " 981: 'mild',\n",
              " 982: 'controlled',\n",
              " 983: 'contract',\n",
              " 984: 'experience',\n",
              " 985: 'difficulties',\n",
              " 986: 'seem',\n",
              " 987: 'great',\n",
              " 988: 'question',\n",
              " 989: 'lung',\n",
              " 990: 'disease',\n",
              " 991: 'carries',\n",
              " 992: 'control',\n",
              " 993: 'handwashwng',\n",
              " 994: 'distancing',\n",
              " 995: 'young',\n",
              " 996: 'age',\n",
              " 997: 'complications',\n",
              " 998: 'diabetes',\n",
              " 999: 'obesity',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUf_PfG6YLmp",
        "colab_type": "code",
        "outputId": "7fbb681c-5195-4d18-b9bb-c3470cc117b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lang.n_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaGfj2m5YLmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}